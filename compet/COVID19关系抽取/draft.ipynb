{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_1': 'Safety and efficacy of intravenous bimagrumab in inclusion body myositis (RESILIENT): a randomised, double-blind, placebo-controlled phase 2b trial\\tBimagrumab showed a good safety profile, relative to placebo, in individuals with inclusion body myositis but did not improve 6MWD. The strengths of our study are that, to the best of our knowledge, it is the largest randomised controlled trial done in people with inclusion body myositis, and it provides important natural history data over 12 months.'}\n",
      "{'train_1': [{'head': {'word': 'Bimagrumab', 'start': 148, 'end': 158}, 'rel': 'PositivelyRegulates', 'tail': {'word': 'inclusion body myositis', 'start': 230, 'end': 253}}]}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import os\n",
    "root = r\"D:\\data\\数据资料\\biendata\\covid19赛道二关系抽取\\task2_public\"\n",
    "train_text = \"task2_user_train.json\"\n",
    "train_label = \"task2_train_label.json\"\n",
    "\n",
    "def openfile(file):\n",
    "    with open(file) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_texts = openfile(os.path.join(root, train_text))\n",
    "train_labels = openfile(os.path.join(root, train_label))\n",
    "\n",
    "print(train_texts[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1326\n",
      "1326\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为用到了transformer的tokenizer， 需要结合分词接过来修改标注开始的地方。\n",
    "# import transformers\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "pretrain_model_weight = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "test_sentence = \"Safety and efficacy of intravenous bimagrumab in inclusion body myositis (RESILIENT): a randomised, double-blind, placebo-controlled phase 2b trial\\tBimagrumab showed a good safety profile, relative to placebo, in individuals with inclusion body myositis but did not improve 6MWD. The strengths of our study are that, to the best of our knowledge, it is the largest randomised controlled trial done in people with inclusion body myositis, and it provides important natural history data over 12 months.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3446"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab.get(\"Here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [9218, 1105, 23891, 1104, 1107, 4487, 7912, 2285, 16516, 1918, 1403, 5697, 6639, 1107, 10838, 1404, 1139, 2155, 10721, 113, 155, 9919, 17656, 17444, 15681, 114, 131, 170, 7091, 3673], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence_2 = \"Safety and efficacy of intravenous bimagrumab in inclusion body myositis (RESILIENT): a randomised\"\n",
    "tokend = tokenizer(test_sentence_2, add_special_tokens=False)\n",
    "tokend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 765])\n",
      "torch.Size([6, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1277])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "batch_size = 6\n",
    "bert_emb_size = 765\n",
    "hidden_size = 512\n",
    "inp = torch.randn(batch_size, bert_emb_size)\n",
    "print(inp.size())\n",
    "hid = torch.randn(batch_size, hidden_size)\n",
    "print(hid.size())\n",
    "\n",
    "torch.cat([inp, hid], dim=1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "model =  AutoModel.from_pretrained(pretrain_model_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "input_ids_tensor = torch.tensor(tokend.get('input_ids'), dtype=torch.long).unsqueeze(dim=0)\n",
    "# print(input_ids_tensor.size())\n",
    "atten_mask_tensor = torch.tensor(tokend.get('attention_mask'), dtype=torch.long).unsqueeze(dim=0)\n",
    "\n",
    "inputs = {\"input_ids\": input_ids_tensor,\n",
    "          \"attention_mask\": atten_mask_tensor}\n",
    "outputs = model(**inputs)\n",
    "# print(outputs.loss.size())\n",
    "print(outputs[0].size())\n",
    "print(outputs[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t_char_and_len(t):\n",
    "    if t.startswith(\"##\"):\n",
    "        return t[2:], len(t)-2\n",
    "    else:\n",
    "        return t, len(t)\n",
    "\n",
    "def chars_token_align(sentence, tokenized):\n",
    "    \"\"\"逐个对齐，\"\"\"\n",
    "    s_idx = 0\n",
    "    token_s_e_idx = []\n",
    "    for idx, t in enumerate(tokenized):\n",
    "        t_char, t_len = get_t_char_and_len(t)\n",
    "        \n",
    "        token_s_idx = sentence.find(t_char, s_idx, s_idx+t_len+3)\n",
    "        if token_s_idx != -1:\n",
    "            token_e_idx = token_s_idx + t_len\n",
    "        else:\n",
    "            print(f\"error at {idx}: {t}\")\n",
    "            break \n",
    "        token_s_e_idx.append((token_s_idx, token_e_idx))\n",
    "\n",
    "        s_idx = token_e_idx\n",
    "    return  token_s_e_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Safety',\n",
       " 'and',\n",
       " 'efficacy',\n",
       " 'of',\n",
       " 'in',\n",
       " '##tra',\n",
       " '##ven',\n",
       " '##ous',\n",
       " 'bi',\n",
       " '##ma',\n",
       " '##g',\n",
       " '##rum',\n",
       " '##ab',\n",
       " 'in',\n",
       " 'inclusion',\n",
       " 'body',\n",
       " 'my',\n",
       " '##os',\n",
       " '##itis',\n",
       " '(',\n",
       " 'R',\n",
       " '##ES',\n",
       " '##IL',\n",
       " '##IE',\n",
       " '##NT',\n",
       " ')',\n",
       " ':',\n",
       " 'a',\n",
       " 'random',\n",
       " '##ised',\n",
       " ',',\n",
       " 'double',\n",
       " '-',\n",
       " 'blind',\n",
       " ',',\n",
       " 'place',\n",
       " '##bo',\n",
       " '-',\n",
       " 'controlled',\n",
       " 'phase',\n",
       " '2',\n",
       " '##b',\n",
       " 'trial',\n",
       " 'B',\n",
       " '##ima',\n",
       " '##g',\n",
       " '##rum',\n",
       " '##ab',\n",
       " 'showed',\n",
       " 'a',\n",
       " 'good',\n",
       " 'safety',\n",
       " 'profile',\n",
       " ',',\n",
       " 'relative',\n",
       " 'to',\n",
       " 'place',\n",
       " '##bo',\n",
       " ',',\n",
       " 'in',\n",
       " 'individuals',\n",
       " 'with',\n",
       " 'inclusion',\n",
       " 'body',\n",
       " 'my',\n",
       " '##os',\n",
       " '##itis',\n",
       " 'but',\n",
       " 'did',\n",
       " 'not',\n",
       " 'improve',\n",
       " '6',\n",
       " '##M',\n",
       " '##WD',\n",
       " '.',\n",
       " 'The',\n",
       " 'strengths',\n",
       " 'of',\n",
       " 'our',\n",
       " 'study',\n",
       " 'are',\n",
       " 'that',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'best',\n",
       " 'of',\n",
       " 'our',\n",
       " 'knowledge',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'largest',\n",
       " 'random',\n",
       " '##ised',\n",
       " 'controlled',\n",
       " 'trial',\n",
       " 'done',\n",
       " 'in',\n",
       " 'people',\n",
       " 'with',\n",
       " 'inclusion',\n",
       " 'body',\n",
       " 'my',\n",
       " '##os',\n",
       " '##itis',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'provides',\n",
       " 'important',\n",
       " 'natural',\n",
       " 'history',\n",
       " 'data',\n",
       " 'over',\n",
       " '12',\n",
       " 'months',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokened = tokenizer.tokenize(test_sentence)\n",
    "test_tokened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_stru:\n",
    "    def __init__(self, string):\n",
    "        self.string = string\n",
    "        self.string_start = None\n",
    "        self.string_end = None \n",
    "        self.token_idx = None \n",
    "        self.token_tag = None \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.token_idx}\\t{self.string}\\t{self.string_start}\\t{self.string_end}\\t{self.token_tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokened_stru = [Token_stru(i) for i in test_tokened]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_s_e_idx = chars_token_align(test_sentence, test_tokened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (stru, s_e_idx) in enumerate(zip(test_tokened_stru, test_token_s_e_idx)):\n",
    "    stru.string_start = s_e_idx[0]\n",
    "    stru.string_end = s_e_idx[1]\n",
    "    stru.token_idx = idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety\n",
      "0\n",
      "6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_tokened_stru[0].string)\n",
    "print(test_tokened_stru[0].string_start)\n",
    "print(test_tokened_stru[0].string_end)\n",
    "print(test_tokened_stru[0].token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'head': {'word': 'Bimagrumab', 'start': 148, 'end': 158},\n",
       "  'rel': 'PositivelyRegulates',\n",
       "  'tail': {'word': 'inclusion body myositis', 'start': 230, 'end': 253}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0].get(\"train_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_head(labels, tokened_struc):\n",
    "    for label in labels:\n",
    "        head_word = label.get(\"head\").get(\"word\")\n",
    "        head_start = label.get(\"head\").get(\"start\")\n",
    "        head_end = label.get(\"head\").get(\"end\")\n",
    "\n",
    "        for j in tokened_struc:\n",
    "            if head_start == j.string_start:\n",
    "                j.token_tag = \"B-head\"\n",
    "            if head_end == j.string_end:\n",
    "                j.token_tag = \"E-head\"\n",
    "            if head_start == head_end:\n",
    "                j.token_tag = \"U-head\"\n",
    "    return tokened_struc\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = train_labels[0].get(\"train_1\")\n",
    "test_tokened_stru = tag_head(test_labels, test_tokened_stru)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "print(len(test_tokened_stru))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_draft.txt\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    for i in test_tokened_stru:\n",
    "        tag = i.token_tag \n",
    "        tag = \"O\" if tag is None else tag\n",
    "        f.write(f\"{i.token_idx}\\t{i.string}\\t{i.string_start}\\t{i.string_end}\\t{tag}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out  = transformer_encoder(src)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "a = os.listdir(\"./data/head\") \n",
    "lenghts = []\n",
    "for i in a:\n",
    "    with open(os.path.join(r\"./data/head\", i), encoding=\"utf-8\") as f:\n",
    "        lenghts.append(len(f.readlines()))\n",
    "\n",
    "import pandas as pd\n",
    "b = pd.Series(lenghts)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1269.000000\n",
       "mean      199.212766\n",
       "std       156.645782\n",
       "min        34.000000\n",
       "25%       100.000000\n",
       "50%       137.000000\n",
       "75%       234.000000\n",
       "max      1169.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "====================\n",
      "1\n",
      "2\n",
      "====================\n",
      "2\n",
      "3\n",
      "====================\n",
      "3\n",
      "4\n",
      "====================\n",
      "4\n",
      "5\n",
      "====================\n",
      "5\n",
      "6\n",
      "====================\n",
      "6\n",
      "7\n",
      "====================\n",
      "7\n",
      "8\n",
      "====================\n",
      "8\n",
      "9\n",
      "====================\n",
      "9\n",
      "10\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "h = 0\n",
    "for i in range(10):\n",
    "    print(h)\n",
    "    h = h+1\n",
    "    print(h)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "batch_size = 6\n",
    "hidden_size = 7\n",
    "feature_size = 9\n",
    "batch1 = torch.randn(batch_size, hidden_size)\n",
    "batch2 = torch.randn(batch_size, feature_size)\n",
    "\n",
    "torch.cat([batch1, batch2],dim=1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 5])\n",
      "torch.Size([72])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 6\n",
    "seq_len = 12\n",
    "num_tags = 5\n",
    "pred = torch.randn(batch_size, seq_len, num_tags)\n",
    "truth = torch.randn(batch_size, seq_len)\n",
    "\n",
    "a = pred.view(-1, num_tags)\n",
    "b = truth.view(-1)\n",
    "\n",
    "print(a.size())\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# torch conv1d用法\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 16\n",
    "seq_len = 512\n",
    "feature_dim = 200\n",
    "tag_num = 10\n",
    "\n",
    "input = torch.randn(batch_size,seq_len,feature_dim)\n",
    "m = nn.Conv1d(feature_dim,tag_num, 1)\n",
    "output = m(input.permute(0,2,1))\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# torch 切片【】\n",
    "a = torch.randn(4,10,5)\n",
    "b = a[:, 1:4, :]\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
